# Lab 2 â€“ Fine-Tuning a Generative AI Model

This lab focuses on fine-tuning a pre-trained generative model for text summarization on a domain-specific dataset.

## Objectives
- Load and preprocess a custom summarization dataset.
- Fine-tune a transformer-based generative model.
- Monitor training metrics and evaluation scores.
- Save and reuse the fine-tuned model for inference.

## Key Concepts
- Transfer Learning with LLMs
- Model Fine-Tuning
- Training & Validation Splits
- Evaluation with ROUGE Metrics

## Technologies
- Python, PyTorch
- Hugging Face Transformers
- Datasets & Evaluate libraries
- PEFT, LoRA

## Output
A fine-tuned summarization model with improved performance over the base pre-trained model.

