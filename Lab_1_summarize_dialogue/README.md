# Lab 1 â€“ Dialogue Summarization

This lab demonstrates how to summarize multi-speaker dialogues using transformer-based Large Language Models (LLMs).

## Objectives
- Load a pre-trained transformer model for summarization.
- Apply the model to multi-speaker dialogue data.
- Generate concise and coherent summaries.
- Evaluate summary quality using standard metrics.

## Key Concepts
- Abstractive Summarization
- Tokenization and Text Preprocessing
- Sequence-to-Sequence Transformer Models
- Inference with Pre-trained LLMs

## Technologies
- Python, PyTorch
- Hugging Face Transformers
- Datasets & Evaluate libraries
- Numpy, Pandas, tqdm

## Output
Automatically generated summaries of dialogues and qualitative comparison with reference summaries.
